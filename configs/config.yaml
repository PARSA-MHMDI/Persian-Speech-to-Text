config: conf/tuning/train_lm_transformer.yaml
print_config: false
log_level: INFO
dry_run: false
iterator_type: sequence
output_dir: exp/lm_train_lm_transformer_fa_bpe150
ngpu: 1
seed: 0
num_workers: 1
num_att_plot: 3
dist_backend: nccl
dist_init_method: env://
dist_world_size: null
dist_rank: null
local_rank: 0
dist_master_addr: null
dist_master_port: null
dist_launcher: null
multiprocessing_distributed: false
unused_parameters: false
sharded_ddp: false
cudnn_enabled: true
cudnn_benchmark: false
cudnn_deterministic: true
collect_stats: false
write_collected_feats: false
max_epoch: 50
patience: null
val_scheduler_criterion:
- valid
- loss
early_stopping_criterion:
- valid
- loss
- min
best_model_criterion:
-   - valid
    - loss
    - min
keep_nbest_models: 10
nbest_averaging_interval: 0
grad_clip: 5.0
grad_clip_type: 2.0
grad_noise: false
accum_grad: 4
no_forward_run: false
resume: true
train_dtype: float32
use_amp: true
log_interval: null
use_matplotlib: true
use_tensorboard: true
create_graph_in_tensorboard: false
use_wandb: false
wandb_project: null
wandb_id: null
wandb_entity: null
wandb_name: null
wandb_model_log_interval: -1
detect_anomaly: false
pretrain_path: null
init_param: []
ignore_init_mismatch: false
freeze_param: []
num_iters_per_epoch: null
batch_size: 20
valid_batch_size: null
batch_bins: 2000000
valid_batch_bins: null
train_shape_file:
- exp/lm_stats_fa_bpe150/train/text_shape.bpe
valid_shape_file:
- exp/lm_stats_fa_bpe150/valid/text_shape.bpe
batch_type: numel
valid_batch_type: null
fold_length:
- 150
sort_in_batch: descending
shuffle_within_batch: false
sort_batch: descending
multiple_iterator: false
chunk_length: 500
chunk_shift_ratio: 0.5
num_cache_chunks: 1024
chunk_excluded_key_prefixes: []
train_data_path_and_name_and_type:
-   - dump/raw/lm_train.txt
    - text
    - text
valid_data_path_and_name_and_type:
-   - dump/raw/org/dev_fa/text
    - text
    - text
allow_variable_data_keys: false
max_cache_size: 0.0
max_cache_fd: 32
valid_max_cache_size: null
exclude_weight_decay: false
exclude_weight_decay_conf: {}
optim: adam
optim_conf:
    lr: 0.002
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 25000
token_list:
- <blank>
- <unk>
- ی
- ه
- ا
- ر
- م
- و
- ▁
- د
- ت
- ش
- ن
- ▁ب
- ز
- ل
- ▁می
- ▁ا
- .
- ان
- ب
- س
- ▁م
- ؟
- گ
- ▁د
- ک
- ید
- ق
- ▁ن
- ▁و
- ار
- ون
- ▁با
- ▁من
- ع
- ▁ک
- ▁ت
- ند
- ست
- ،
- ف
- ج
- ▁به
- ▁س
- خ
- ▁پ
- ▁خ
- ▁این
- ▁کن
- ▁را
- ▁در
- ین
- ▁تو
- ▁ش
- ▁دار
- ▁از
- ▁است
- ای
- اد
- ط
- ح
- ▁که
- ال
- ▁چ
- ور
- ▁رو
- ▁گ
- ▁ج
- ▁ف
- ▁آ
- ▁هم
- یم
- ام
- ▁ح
- فت
- ▁یک
- ص
- ▁ق
- ▁بر
- ▁ع
- ▁کرد
- لا
- ▁ما
- ▁بود
- ▁هست
- اب
- کن
- پ
- ▁کار
- غ
- چ
- ذ
- ض
- ظ
- '!'
- ث
- ً
- '"'
- ئ
- ژ
- ك
- ي
- ':'
- آ
- ى
- '-'
- أ
- ِ
- »
- ','
- ـ
- (
- )
- ُ
- ء
- ٬
- ٔ
- َ
- B
- ؛
- ّ
- C
- E
- G
- M
- S
- ؤ
- F
- I
- _
- H
- T
- D
- K
- –
- U
- ٌ
- P
- ;
- Q
- O
- Z
- N
- Y
- '#'
- A
- '&'
- «
- <sos/eos>
init: null
model_conf:
    ignore_id: 0
use_preprocessor: true
token_type: bpe
bpemodel: data/fa_token_list/bpe_unigram150/bpe.model
non_linguistic_symbols: null
cleaner: null
g2p: null
lm: transformer
lm_conf:
    pos_enc: null
    embed_unit: 128
    att_unit: 512
    head: 8
    unit: 2048
    layer: 16
    dropout_rate: 0.1
required:
- output_dir
- token_list
version: '202304'
distributed: false
